# A decoder only Transformer implementation

The Transformer implemented in this repo is a Decoder only transformer which uses masked attention. Meaning the tokens in the training phase do not see the tokens in the future by implementing a masked attention block.

Apart from the Transformer implementation, the repo also includes other modules which will be a requirement for building a base language model end to end.

Inspired from [@karpathy](https://github.com/karpathy)'s nanogpt implementation :raised_hands:

## Module: Tokenizer

A Tokenizer class capable of taking a parent dataset and tokenize a stream of characters or a string. It can also convert back the tokenized indices to the original string or stream of characters.


### Codeblock:

```
tokenizer = Tokenizer(dataset_path='./dataset/shakespear-text.txt')
dataset = tokenizer.get_dataset()

print(dataset[:200])

encoded_value = tokenizer.encode('Hey How are You')
print(encoded_value)
print(tokenizer.decode(encoded_value))
print(tokenizer.decode(encoded_value, True))
```

### Output

```
>   First Citizen:
    Before we proceed any further, hear me speak.

    All:
    Speak, speak.

    First Citizen:
    You are all resolved rather to die than to famish?

    All:
    Resolved. resolved.

    First Citizen:
    First, you
>   [20, 43, 63, 1, 20, 53, 61, 1, 39, 56, 43, 1, 37, 53, 59]
>   ['H', 'e', 'y', ' ', 'H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'Y', 'o', 'u']
>   Hey How are You
```

## Module: Trainer

Trainer module can be used to get the train-test split for a provided split value. A split value of `0.9` means `90%` of the data is for training and the rest `10%` for validation.


### Codeblock

```
trainer = Trainer(
    batch_size=3, block_size=8, train_test_split=0.9, data=tokenizer.encode(dataset)
)

xb, yb = trainer.get_batch_of_train_or_test_split(split='Train')
print(f'Input: {xb}')
print(f'Target: {yb}')

print('Decoded the batch')

print(f'Input : {[tokenizer.decode(x, stringify=True) for x in xb.numpy()]}')
print(f'Target: {[tokenizer.decode(y, stringify=True) for y in yb.numpy()]}')
```


### Output

```
>   Input: tensor([[46, 43, 43, 49,  6,  0, 16, 39],
            [61,  1, 47, 57,  1, 57, 46, 39],
            [53, 42,  1, 51, 39, 57, 58, 43]])
>   Target: tensor([[43, 43, 49,  6,  0, 16, 39, 57],
            [ 1, 47, 57,  1, 57, 46, 39, 56],
            [42,  1, 51, 39, 57, 58, 43, 56]])
>   Decoded the batch
>   Input : ['heek,\nDa', 'w is sha', 'od maste']
>   Target: ['eek,\nDas', ' is shar', 'd master']
```